# 스파크 맛보기
로컬 PC 에서 간단한 스파크 프로그램을 작성하여 돌려본다.
MAC 사용, 로컬 모드라 얀, 메소스와 같은 리소스 관리 툴은 사용하지 않았다.

## 다운로드 이후 프로파일 설정
![profile](https://github.com/cacacoo/stack/blob/master/resource/images/spark_2_1_profile.png)

## 스파크 쉘 실행 : spark-shell
![sample](https://github.com/cacacoo/stack/blob/master/resource/images/spark_2_2_sample.png)

스파크에서는 연산 과정을 클러스터 전체에 걸쳐 자동으로 병렬화해 분산 배치된 연산 작업들의 모음으로 표현한다. 이 모음은 탄력적인 분산 데이터 세트 RDD(Resilient Distributed Dataset) 이라 부른다. RDD 는 분산 데이터와 연산을 위한 스파크의 핵심개념이다.

### 줄 수 세기 예제
```
scala> val lines = sc.textFile("README.md")
lines: org.apache.spark.rdd.RDD[String] = README.md MapPartitionsRDD[1] at textFile at <console>:24

scala> lines.count()
res0: Long = 105

scala> lines.first()
res1: String = # Apache Spark
```

### 드라이버 프로그램
스파크 애플리케이션은 단순하게보면 클러스터에서 병렬 연산을 수행하는 드라이버 프로그램으로 구성된다.드라이버 프로그램은 애플리케이션 main 함수를 가지고있으며, RDD를 정의하고 RDD에 연산 작업을 수행하는
 역할을 한다. 이런 드라이버 프로그램들은 연산 클러스터에 대한 연결을 나타내는 SparkContext 객체를 통해 스파크에 접속한다.
연산 수행을 위해 드라이버 프로그램들은 보통 executor 라 불리는 다수의 노드를 관리한다.

쉘에서는 sc가 이미 선언되어 있다.
```
scala> sc
res2: org.apache.spark.SparkContext = org.apache.spark.SparkContext@30dbcffd
```





